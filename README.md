# Segmentation-Informed-Captioning-A-Multi-Stage-Pipeline-for-Surgical-Vision-Language-

Abstract: General surgical understanding aims to develop models that generalize across procedures and tasks, in contrast to fully-supervised, task-specific approaches. Recent work has explored VLMs for this purpose. However, their effectiveness–particularly on tasks requiring fine-grained scene understanding–is often constrained by noisy and misaligned datasets, typically based on transcribed audio. In this paper, we propose a five-stage pipeline to construct more accurate and less noisy vision-language datasets from existing segmentation datasets. Our method applies rule-based heuristics to extract spatial, and interaction cues, which are then used to prompt a large language model (LLM) to produce naturally sound, clinically coherent captions. Evaluation by three medical experts on how well the captions met stage-specific expectation found that 95% of the generated captions scored or higher on a Likert scale.


